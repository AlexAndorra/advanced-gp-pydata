{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlexAndorra/advanced-gp-pydata/blob/master/notebooks/gaussian_processes.ipynb)\n",
    "\n",
    "# Mastering Gaussian Processes with PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import preliz as pz\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cmap\n",
    "sns.set_context('notebook')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DATA_URL = 'https://raw.githubusercontent.com/fonnesbeck/advanced-gp-pydata/master/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Most of us are familiar with linear regression models, which are a type of *parametric* model. In a linear regression model, we assume that the relationship between the input variables $x$ and the output variable $y$ is linear, and we model the relationship using a linear combination of the input variables.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon$$\n",
    "\n",
    "where $\\beta_0, \\beta_1, \\ldots, \\beta_p$ are the model parameters and $\\epsilon$ is the error term. The model parameters specify a line in the $p$-dimensional space of input variables, and the error term allows for the data to deviate from the linear prediction. Even if we have data that describe non-linear relationships, we can often use a model that is linear in its parameters by expanding the space of input variables using basis functions.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 \\phi_1(x_1) + \\beta_2 \\phi_2(x_2) + \\cdots + \\beta_p \\phi_p(x_p) + \\epsilon$$\n",
    "\n",
    "where $\\phi_1(x_1), \\phi_2(x_2), \\ldots, \\phi_p(x_p)$ are the basis functions. The simplest example of this is polynomial regression, where the basis functions are powers of the input variable:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_p x^p + \\epsilon$$\n",
    "\n",
    "The limitation of parametric, linear models is that decisions about the form of the relationship between $x$ and $y$ must be made *before* the data have been observed. We can only ever hope to approximate the true relationship, and the quality of the approximation will depend on the choices we made about the form of the relationship.\n",
    "\n",
    "An alternative to parametric models are *non-parametric* models, which do not make any assumptions about the form of the relationship between $x$ and $y$. Instead, we can use a Gaussian process to model an arbitrary smooth, continuous relationship between $x$ and $y$. Rather than modeling parameters associated with a latent function, these methods predict the function directly.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "Use of the term \"non-parametric\" in the context of Bayesian analysis is something of a misnomer. This is because the first and fundamental step in Bayesian modeling is to specify a *full probability model* for the problem at hand. It is rather difficult to explicitly state a full probability model without the use of probability functions, which are parametric. Bayesian non-parametric methods do not imply that there are no parameters, but rather that the number of parameters grows with the size of the dataset. In fact, Bayesian non-parametric models are *infinitely* parametric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models with Gaussians\n",
    "\n",
    "What if we chose to use Gaussian distributions to model our data? \n",
    "\n",
    "$$p(x \\mid \\pi, \\Sigma) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2} \\exp\\left\\{ -\\frac{1}{2} (x-\\mu)^{\\prime}\\Sigma^{-1}(x-\\mu) \\right\\}$$\n",
    "\n",
    "There would not seem to be an advantage to doing this, because normal distributions are not particularly flexible distributions in and of themselves. However, adopting a set of Gaussians (a multivariate normal vector) confers a number of advantages. First, the marginal distribution of any subset of elements from  a multivariate normal distribution is also normal:\n",
    "\n",
    "$$p(x,y) = \\mathcal{N}\\left(\\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\mu_x}  \\\\\n",
    "  {\\mu_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right], \\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\Sigma_x} & {\\Sigma_{xy}}  \\\\\n",
    "  {\\Sigma_{xy}^T} & {\\Sigma_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right]\\right)$$\n",
    "\n",
    "$$p(x) = \\int p(x,y) dy = \\mathcal{N}(\\mu_x, \\Sigma_x)$$\n",
    "\n",
    "Also, conditionals distributions of a subset of a multivariate normal distribution (conditional on the remaining elements) are normal too:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is defined as an infinite collection of random variables, any finite subset of which have a Gaussian distribution. Thus, the marginalization property is explicit in its definition. Another way of thinking about an infinite vector is as a *function*. When we write a function that takes continuous values as inputs, we are essentially specifying an infinte vector that only returns values (indexed by the inputs) when the function is called upon to do so. By the same token, this notion of an infinite-dimensional Gaussian as a function allows us to work with them computationally: we are never required to store all the elements of the Gaussian process, only to calculate them on demand.\n",
    "\n",
    "So, we can describe a Gaussian process as a ***disribution over functions***. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a **mean function** and a **covariance function**:\n",
    "\n",
    "$$p(x) \\sim \\mathcal{GP}(m(x), k(x,x^{\\prime}))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed. \n",
    "\n",
    "For example, one specification of a GP might be as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m(x) &=0 \\\\\n",
    "k(x,x^{\\prime}) &= \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "here, the covariance function is a **squared exponential**, for which values of $x$ and $x^{\\prime}$ that are close together result in values of $k$ closer to 1 and those that are far apart return values closer to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_cov(x, y, scale, length_scale):\n",
    "    return scale * np.exp( -0.5 * length_scale * np.subtract.outer(x, y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "xrange = np.linspace(0, 5)\n",
    "ax1.plot(xrange, exponential_cov(0, xrange, 1, 1))\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('cov(0, x)')\n",
    "\n",
    "z = np.array([exponential_cov(xrange, xprime, 1, 1) for xprime in xrange])\n",
    "ax2.imshow(z, cmap=\"inferno\", \n",
    "       interpolation='none', \n",
    "       extent=(0, 5, 5, 0))\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('x')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem odd to simply adopt the zero function to represent the mean function of the Gaussian process -- surely we can do better than that! It turns out that most of the learning in the GP involves the covariance function and its parameters, so very little is gained in specifying a complicated mean function.\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a Gaussian Process Prior\n",
    "\n",
    "To make this notion of a \"distribution over functions\" more concrete, let's quickly demonstrate how we obtain realizations from a Gaussian process, which result in an evaluation of a function over a set of points. All we will do here is sample from the *prior* Gaussian process, so before any data have been introduced. What we need first is our covariance function, which will be the squared exponential, and a function to evaluate the covariance at given points (resulting in a covariance matrix)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going generate realizations sequentially, point by point, using the lovely conditioning property of mutlivariate Gaussian distributions. Here is that conditional:\n",
    "\n",
    "$$p(y^*| x^*, y, x) = \\mathcal{N}(\\Sigma_{x^*x}\\Sigma_y^{-1}y,\\>\n",
    "\\Sigma_{x^*}-\\Sigma_{x^*x}\\Sigma_y^{-1}\\Sigma_{x^*x}^T)$$\n",
    "\n",
    "And this the function that implements it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional(x_new, x, y, scale, length_scale):\n",
    "    B = exponential_cov(x_new, x, scale, length_scale)\n",
    "    C = exponential_cov(x, x, scale, length_scale)\n",
    "    A = exponential_cov(x_new, x_new, scale, length_scale)\n",
    "    mu = np.linalg.inv(C).dot(B.T).T.dot(y)\n",
    "    sigma = A - B.dot(np.linalg.inv(C).dot(B.T))\n",
    "    return(mu.squeeze(), sigma.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a Gaussian process prior with hyperparameters $\\theta_0=1, \\theta_1=10$. We will also assume a zero function as the mean, so we can plot a band that represents one standard deviation from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, length_scale = 1, 10\n",
    "sigma_0 = exponential_cov(0, 0, scale, length_scale)\n",
    "xpts = np.arange(-3, 3, step=0.01)\n",
    "plt.errorbar(xpts, np.zeros(len(xpts)), yerr=sigma_0, capsize=0)\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select an arbitrary starting point to sample, say $x=1$. Since there are no prevous points, we can sample from an unconditional Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.]\n",
    "y = [np.random.normal(scale=sigma_0)]\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update our confidence band, given the point that we just sampled, using the covariance function to generate new point-wise intervals, conditional on the value $[x_0, y_0]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_1 = exponential_cov(x, x, scale, length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, data, kernel, scale, length_scale, sigma, t):\n",
    "    k = [kernel(x, y, scale, length_scale) for y in data]\n",
    "    Sinv = np.linalg.inv(sigma)\n",
    "    y_pred = np.dot(k, Sinv).dot(t)\n",
    "    sigma_new = kernel(x, x, scale, length_scale) - np.dot(k, Sinv).dot(k)\n",
    "    return y_pred, sigma_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-3, 3, 1000)\n",
    "predictions = [predict(i, x, exponential_cov, scale, length_scale, sigma_1, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So conditional on this point, and the covariance structure we have specified, we have essentially constrained the probable location of additional points. Let's now sample another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = conditional([-0.7], x, y, scale, length_scale)\n",
    "y2 = np.random.normal(m, s)\n",
    "y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is added to the realization, and can be used to further update the location of the next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append(-0.7)\n",
    "y.append(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_2 = exponential_cov(x, x, scale, length_scale)\n",
    "\n",
    "predictions = [predict(i, x, exponential_cov, scale, length_scale, sigma_2, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, sampling sequentially is just a heuristic to demonstrate how the covariance structure works. We can just as easily sample several points at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_more = [-2.1, -1.5, 0.3, 1.8, 2.5]\n",
    "mu, s = conditional(x_more, x, y, scale, length_scale)\n",
    "y_more = np.random.multivariate_normal(mu, s)\n",
    "y_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x += x_more\n",
    "y += y_more.tolist()\n",
    "\n",
    "sigma_new = exponential_cov(x, x, scale, length_scale)\n",
    "\n",
    "predictions = [predict(i, x, exponential_cov, scale, length_scale, sigma_new, y) for i in x_pred]\n",
    "\n",
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as the density of points becomes high, the result will be one realization (function) from the prior GP. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes regression\n",
    "\n",
    "The following simulated data clearly shows some type of non-linear process, corrupted by a certain amount of observation or measurement error so it should be a reasonable task for a Gaussian process approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100 # The number of data points\n",
    "x = np.linspace(0, 10, n)\n",
    "X = x[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "l_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, l_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of PyTensor\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "    cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "sigma_true = 2.0\n",
    "y = f_true + sigma_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Likelihood Implementation\n",
    "\n",
    "The `gp.Marginal` class in PyMC implements the simplest case of GP regression:  the observed data are the sum of a GP and Gaussian noise.  `gp.Marginal` has a `marginal_likelihood` method, a `conditional` method, and a `predict` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$\n",
    "\n",
    "The observations $y$ are the unknown function plus noise\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\epsilon &\\sim N(0, \\Sigma) \\\\\n",
    "  y &= f(x) + \\epsilon \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### The Marginal Likelihood\n",
    "\n",
    "The marginal likelihood is the normalizing constant for the posterior distribution, and is the integral of the product of the likelihood and prior.\n",
    "\n",
    "$$p(y|X) = \\int_f p(y|f,X)p(f|X) df$$\n",
    "\n",
    "where for Gaussian processes, we are marginalizing over function values $f$ (instead of parameters $\\theta$).\n",
    "\n",
    "**GP prior**:\n",
    "\n",
    "$$\\log p(f|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K| -\\frac{1}{2}f^TK^{-1}f $$\n",
    "\n",
    "**Gaussian likelihood**:\n",
    "\n",
    "$$\\log p(y|f,X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|\\sigma^2I| -\\frac{1}{2}(y-f)^T(\\sigma^2I)^{-1}(y-f) $$\n",
    "\n",
    "**Marginal likelihood**:\n",
    "\n",
    "$$\\log p(y|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K + \\sigma^2I| - \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y $$\n",
    "\n",
    "Notice that the marginal likelihood includes both a data fit term $- \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y$ and a parameter penalty term $\\frac{1}{2}\\log|K + \\sigma^2I|$. Hence, the marginal likelihood can help us select an appropriate covariance function, based on its fit to the dataset at hand.\n",
    "\n",
    "### Choosing parameters\n",
    "\n",
    "This is relevant because we have to make choices regarding the parameters of our Gaussian process; they were chosen arbitrarily for the random functions we demonstrated above.\n",
    "\n",
    "For example, in the squared exponential covariance function, we must choose two parameters:\n",
    "\n",
    "$$k(x,x^{\\prime}) = \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)$$\n",
    "\n",
    "The first parameter $\\theta_1$ is a scale parameter, which allows the function to yield values outside of the unit interval. The second parameter $\\theta_2$ is a length scale parameter that determines the degree of covariance between $x$ and $x^{\\prime}$; smaller values will tend to smooth the function relative to larger values.\n",
    "\n",
    "We can use the **marginal likelihood** to select appropriate values for these parameters, since it trades off model fit with model complexity. Thus, an optimization procedure can be used to select values for $\\theta$ that maximize the marginial likelihood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance functions\n",
    "\n",
    "The behavior of individual realizations from the GP is governed by the covariance function. This function controls both the degree of *shrinkage* to the mean function and the *smoothness* of functions sampled from the GP.\n",
    "\n",
    "PyMC includes a library of covariance functions to choose from. A flexible choice to start with is the Mat&#232;rn covariance. \n",
    "\n",
    "$$k_{M}(x) = \\frac{\\sigma^2}{\\Gamma(\\nu)2^{\\nu-1}} \\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)^{\\nu} K_{\\nu}\\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)$$\n",
    "\n",
    "where where $\\Gamma$ is the gamma function and $K$ is a modified Bessel function. The form of covariance matrices sampled from this function is governed by three parameters, each of which controls a property of the covariance.\n",
    "\n",
    "* **amplitude** ($\\sigma$) controls the scaling of the output along the y-axis. This parameter is just a scalar multiplier, and is therefore usually left out of implementations of the Mat&#232;rn function (*i.e.* set to one)\n",
    "\n",
    "* **lengthscale** ($l$) complements the amplitude by scaling realizations on the x-axis. Larger values make points appear closer together.\n",
    "\n",
    "* **roughness** ($\\nu$) controls the sharpness of ridges in the covariance function, which ultimately affect the roughness (smoothness) of realizations.\n",
    "\n",
    "Though in general all the parameters are non-negative real-valued, when $\\nu = p + 1/2$ for integer-valued $p$, the function can be expressed partly as a polynomial function of order $p$ and generates realizations that are $p$-times differentiable, so values $\\nu \\in \\{3/2, 5/2\\}$ are extremely common."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide an idea regarding the variety of forms or covariance functions, here's small selection of available ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.linspace(0,2,200)[:,None]\n",
    "\n",
    "# function to display covariance matrices\n",
    "def plot_cov(X, K, stationary=True):\n",
    "    K = K + 1e-8*np.eye(X.shape[0])\n",
    "    x = X.flatten()\n",
    "    \n",
    "    with sns.axes_style(\"white\"):\n",
    "\n",
    "        fig = plt.figure(figsize=(14,5))\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        m = ax1.imshow(K, cmap=\"inferno\", \n",
    "                       interpolation='none', \n",
    "                       extent=(np.min(X), np.max(X), np.max(X), np.min(X))); \n",
    "        plt.colorbar(m);\n",
    "        ax1.set_title(\"Covariance Matrix\")\n",
    "        ax1.set_xlabel(\"X\")\n",
    "        ax1.set_ylabel(\"X\")\n",
    "\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        if not stationary:\n",
    "            ax2.plot(x, np.diag(K), \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"The Diagonal of K\")\n",
    "            ax2.set_ylabel(\"k(x,x)\")\n",
    "        else:\n",
    "            ax2.plot(x, K[:,0], \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"K as a function of x - x'\")\n",
    "            ax2.set_ylabel(\"k(x,x')\")\n",
    "        ax2.set_xlabel(\"X\")\n",
    "\n",
    "        fig = plt.figure(figsize=(14,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        samples = np.random.multivariate_normal(np.zeros(200), K, 5).T;\n",
    "        for i in range(samples.shape[1]):\n",
    "            ax.plot(x, samples[:,i], color=cmap.inferno(i*0.2), lw=2);\n",
    "        ax.set_title(\"Samples from GP Prior\")\n",
    "        ax.set_xlabel(\"X\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic exponential covariance\n",
    "\n",
    "This is the squared exponential covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2 \n",
    "    tau = 2.0\n",
    "    b = 0.5\n",
    "    cov = b + tau * pm.gp.cov.ExpQuad(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matern $\\nu=3/2$ covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Matern12(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Cosine(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a general idea about covariance functions, let's begin by defining one for our first model.\n",
    "\n",
    "We can use a Matern(5/2) covariance to model our simulated data, and pass this as the `cov_func` argument to the `Marginal` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    l = pm.Gamma(\"l\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, l)\n",
    "    mean = pm.gp.mean.Constant(c=1)\n",
    "    gp = pm.gp.Marginal(mean_func=mean, cov_func=cov)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.marginal_likelihood` method\n",
    "\n",
    "The unknown latent function can be analytically integrated out of the product of the GP prior probability with a normal likelihood.  This quantity is called the marginal likelihood. \n",
    "\n",
    "$$\n",
    "p(y \\mid x) = \\int p(y \\mid f, x) \\, p(f \\mid x) \\, df\n",
    "$$\n",
    "\n",
    "The log of the marginal likelihood, $p(y \\mid x)$, is\n",
    "\n",
    "$$\n",
    "\\log p(y \\mid x) = \n",
    "  -\\frac{1}{2} (\\mathbf{y} - \\mathbf{m}_x)^{T} \n",
    "               (\\mathbf{K}_{xx} + \\boldsymbol\\Sigma)^{-1} \n",
    "               (\\mathbf{y} - \\mathbf{m}_x)\n",
    "  - \\frac{1}{2}|\\mathbf{K}_{xx} + \\boldsymbol\\Sigma|\n",
    "  - \\frac{n}{2}\\log (2 \\pi)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol\\Sigma$ is the covariance matrix of the Gaussian noise.  Since the Gaussian noise doesn't need to be white to be conjugate, the `marginal_likelihood` method supports either using a white noise term when a scalar is provided, or a noise covariance function when a covariance function is provided.\n",
    "\n",
    "The `gp.marginal_likelihood` method implements the quantity given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1)\n",
    "\n",
    "with model:\n",
    "    \n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=5)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X, y=y, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    marginal_post = pm.sample(500, tune=2000, nuts_sampler='numpyro', chains=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can collect the results into a pandas dataframe to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(marginal_post, var_names=[\"l\", \"eta\", \"sigma\"], round_to=2, kind=\"stats\")\n",
    "summary[\"True value\"] = [l_true, eta_true, sigma_true]\n",
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.conditional` distribution\n",
    "\n",
    "In addition to fitting the model, we would like to be able to generate predictions. This implies sampling from the posterior predictive distribution, which if you recall is just some linear algebra:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m^*(x^*) &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k^*(x^*) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "PyMC allows for predictive sampling after the model is fit, using the recorded values of the model parameters to generate samples. The `conditional` method implements the predictive GP above, called with a grid of points over which to generate realizations:\n",
    "\n",
    "The `.conditional` has an optional flag for `pred_noise`, which defaults to `False`.  When `pred_noise=False`, the `conditional` method produces the predictive distribution for the underlying function represented by the GP.  When `pred_noise=True`, the `conditional` method produces the predictive distribution for the GP plus noise.  \n",
    "\n",
    "If using an additive GP model, the conditional distribution for individual components can be constructed by setting the optional argument `given`.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a grid of new values from `x=0` to `x=20`, then add the GP conditional to the model, given the new X values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 20, 600)[:,None]\n",
    "\n",
    "with model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw samples from the posterior predictive distribution over the specified grid of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pred_samples = pm.sample_posterior_predictive(\n",
    "        marginal_post.sel(draw=slice(0, 20)), var_names=['f_pred']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "f_pred_samples = az.extract(pred_samples, group=\"posterior_predictive\", var_names=[\"f_pred\"])\n",
    "plot_gp_dist(ax, samples=f_pred_samples.T, x=X_new);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(x, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(x, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylim([-13,13]);\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction also matches the results from `gp.Latent` very closely.  What about predicting new data points?  Here we only predicted $f_*$, not $f_*$ + noise, which is what we actually observe.\n",
    "\n",
    "The `conditional` method of `gp.Marginal` contains the flag `pred_noise` whose default value is `False`.  To draw from the *posterior predictive* distribution, we simply set this flag to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    y_pred = gp.conditional(\"y_pred\", X_new, pred_noise=True)\n",
    "    y_samples = pm.sample_posterior_predictive(marginal_post.sel(draw=slice(0, 20)), var_names=['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# posterior predictive distribution\n",
    "y_pred_samples = az.extract(y_samples, group=\"posterior_predictive\", var_names=[\"y_pred\"])\n",
    "plot_gp_dist(ax, y_pred_samples.T, X_new, plot_samples=False, palette=\"bone_r\");\n",
    "\n",
    "# overlay a scatter of one draw of random points from the \n",
    "#   posterior predictive distribution\n",
    "plt.plot(X_new, y_pred_samples.values.T[1], \"co\", ms=2, label=\"Predicted data\");\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"posterior predictive distribution, y_*\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world example: Spawning salmon\n",
    "\n",
    "That was contrived data; let's try applying Gaussian processes to a real problem. The plot below shows the relationship between the number of spawning salmon in a particular stream and the number of fry that are recruited into the population in the spring.\n",
    "\n",
    "We would like to model this relationship, which appears to be non-linear (we have biological knowledge that suggests it should be non-linear too).\n",
    "\n",
    "![](images/spawn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    salmon_data = pd.read_table('../data/salmon.txt', sep='\\s+', index_col=0)\n",
    "except FileNotFoundError:\n",
    "    salmon_data = pd.read_table(DATA_URL + 'salmon.txt', sep='\\s+', index_col=0)\n",
    "    \n",
    "salmon_data.plot.scatter(x='spawners', y='recruits', s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as salmon_model:\n",
    "\n",
    "    # Lengthscale\n",
    "    rho = pm.LogNormal('rho', 0, 1)\n",
    "    eta = pm.LogNormal('eta', 0, 1)\n",
    "    \n",
    "    M = pm.gp.mean.Linear(coeffs=(salmon_data.recruits/salmon_data.spawners).mean())\n",
    "    K = (eta**2) * pm.gp.cov.ExpQuad(1, rho) \n",
    "    \n",
    "    sigma = pm.HalfNormal('sigma', 5)\n",
    "    \n",
    "    recruit_gp = pm.gp.Marginal(mean_func=M, cov_func=K)\n",
    "    recruit_gp.marginal_likelihood('recruits', X=salmon_data.spawners.values.reshape(-1,1), y=salmon_data.recruits.values, noise=sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with salmon_model:\n",
    "    map_est = pm.find_MAP()\n",
    "    print(map_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with salmon_model:\n",
    "    salmon_trace = pm.sample(1000, tune=2000, nuts_sampler='numpyro', chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(salmon_trace, var_names=['rho', 'eta', 'sigma']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.linspace(0, 500, 100).reshape(-1, 1)\n",
    "with salmon_model:\n",
    "    salmon_pred = recruit_gp.conditional(\"salmon_pred\", X_pred)\n",
    "    salmon_samples = pm.sample_posterior_predictive(salmon_trace.sel(draw=slice(0, 5)), var_names=[\"salmon_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = salmon_data.plot.scatter(x='spawners', y='recruits', c='k', s=50)\n",
    "ax.set_ylim(0, None)\n",
    "for x in az.extract(salmon_samples.posterior_predictive)['salmon_pred'].values.T:\n",
    "    ax.plot(X_pred, x, alpha=0.5, c='r');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.predict`\n",
    "\n",
    "We can use the `.predict` method to return the mean and variance given a particular `point`.  Since we used `find_MAP` in this example, `predict` returns the same mean and covariance that the distribution of `.conditional` has.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "with model:\n",
    "    mu, var = gp.predict(X_new, point=az.extract(marginal_post.posterior.sel(draw=[0])).squeeze(), diag=True)\n",
    "sd = np.sqrt(var)\n",
    "\n",
    "# draw plot\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot mean and 2sigma intervals\n",
    "plt.plot(X_new, mu, 'r', lw=2, label=\"mean and 2sigma region\");\n",
    "plt.plot(X_new, mu + 2*sd, 'r', lw=1); plt.plot(X_new, mu - 2*sd, 'r', lw=1);\n",
    "plt.fill_between(X_new.flatten(), mu - 2*sd, mu + 2*sd, color=\"r\", alpha=0.5)\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"predictive mean and 2sigma interval\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Implementation\n",
    "\n",
    "The `gp.Latent` class is a more general implementation of a GP.  It is called \"Latent\" because the underlying function values are treated as latent variables.  It has a `prior` method, and a `conditional` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.prior`\n",
    "\n",
    "With some data set of finite size, the `prior` method places a multivariate normal prior distribution on the vector of function values, $\\mathbf{f}$,\n",
    "\n",
    "$$\n",
    "\\mathbf{f} \\sim \\text{MvNormal}(\\mathbf{m}_{x},\\, \\mathbf{K}_{xx}) \\,,\n",
    "$$\n",
    "\n",
    "where the vector $\\mathbf{m}$ and the matrix $\\mathbf{K}_{xx}$ are the mean vector and covariance matrix evaluated over the inputs $x$.  \n",
    "\n",
    "By default, PyMC reparameterizes the prior on `f` under the hood by rotating it with the Cholesky factor of its covariance matrix.  This helps to reduce covariances in the posterior of the transformed random variable, `v`.  The reparameterized model is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathbf{v} \\sim \\text{N}(0, 1)& \\\\\n",
    "  \\mathbf{L} = \\text{Cholesky}(\\mathbf{K}_{xx})& \\\\\n",
    "  \\mathbf{f} = \\mathbf{m}_{x} + \\mathbf{Lv} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This reparameterization can be disabled by setting the optional flag in the `prior` method, `reparameterize = False`.  The default is `True`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust regession\n",
    "\n",
    "The following is an example showing how to specify a simple model with a GP prior using the `gp.Latent` class.  So we can verify that the inference we perform is correct, the data set is made using a draw from a GP.  This will be identical to the first example, except that the noise is Student-T distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100 # The number of data points\n",
    "X = np.linspace(0, 10, n)[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ls_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ls_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of PyTensor\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of T distributed noise\n",
    "# The standard deviation of the noise is `sigma`, and the degrees of freedom is `nu`\n",
    "sigma_true = 2.0\n",
    "nu_true = 3.0\n",
    "y = f_true + sigma_true * np.random.standard_t(nu_true, size=n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the model in PyMC.  We use a $\\text{Gamma}(2, 1)$ prior over the lengthscale parameter, and weakly informative $\\text{HalfCauchy}(2)$ priors over the covariance function scale, and noise scale.  A $\\text{Gamma}(2, 0.1)$ prior is assigned to the degrees of freedom parameter of the noise.  Finally, a GP prior is placed on the unknown function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=2)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls)\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    \n",
    "    f = gp.prior(\"f\", X=X)\n",
    "    \n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=2)\n",
    "    nu = pm.Gamma(\"nu\", alpha=2, beta=0.1)\n",
    "    y_ = pm.StudentT(\"y\", mu=f, lam=1.0/sigma, nu=nu, observed=y)\n",
    "    \n",
    "    trace = pm.sample(nuts_sampler='numpyro', chains=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the posteriors of the covariance function hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"eta\", \"sigma\", \"ls\", \"nu\"], backend_kwargs=dict(constrained_layout=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "plot_gp_dist(ax, trace.posterior[\"f\"].values[0], X);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylabel(\"True f(x)\"); \n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Coal mining disasters\n",
    "\n",
    "The number of coal mining disasters in the UK from 1851 to 1962 is thought to follow a Poisson process with a large rate parameter in the early years and a smaller rate in the later years. We previously modeled this data using a piecewise constant Poisson mean. Use the latent variable implementation to fit a GP to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters_array = np.array(\n",
    "    [4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3, \n",
    "     1, 4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, \n",
    "     1, 3, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, \n",
    "     0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 1, 2, 1, 1, 1, \n",
    "     1, 2, 4, 2, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, \n",
    "     1])\n",
    "years = np.arange(1851, 1962, dtype=int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.bar(years, disasters_array, color=\"#348ABD\", alpha=0.65, width=0.7)\n",
    "ax.set_xlim(years[0], years[-1] + 1)\n",
    "ax.set_ylabel(\"Disaster count\")\n",
    "ax.set_xlabel(\"Year\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Gaussian processes\n",
    "\n",
    "One of the major constraints that limits the utility of Gaussian processes in practice is the inversion of $K$ when calculating the posterior covariance. Since it is evaluated at every observed data point, its execution time is $\\mathcal{O(n^3)}$, which makes Gaussian processes (in the form I have presented here) impractical for larger datasets.\n",
    "\n",
    "An approach for dealing with this computation complexity is to look for an approximation to accelerate training and prediction. For Gaussian processes, this can be accomplished by employing a **sparse approximation** to the Gram matrix that places $M<<N$ *inducing points* along the range of the input variables, and uses this to estimate the full covariance matrix for the observed points. \n",
    "\n",
    "The `gp.MarginalSparse` class implements sparse, or inducing point, GP approximations.  It works identically to `gp.Marginal`, except it additionally requires the locations of the inducing points (denoted `Xu`), and it accepts the argument `sigma` instead of `noise` because these sparse approximations assume white IID noise.\n",
    "\n",
    "The downside of sparse approximations is that they reduce the expressiveness of the GP.  Reducing the dimension of the covariance matrix effectively reduces the number of covariance matrix eigenvectors that can be used to fit the data.  \n",
    "\n",
    "A choice that needs to be made is where to place the inducing points.  One option is to use a subset of the inputs.  Another possibility is to use K-means.  The location of the inducing points can also be an unknown and optimized as part of the model.  These sparse approximations are useful for speeding up calculations when the density of data points is high and the lengthscales is larger than the separations between inducing points. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense dataset\n",
    "\n",
    "For the following examples, we use the same data set as was used in the `gp.Marginal` example, but with more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 2000 # The number of data points\n",
    "X = 10*np.sort(np.random.rand(n))[:,None]\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ls_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ls_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of PyTensor\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "sigma_true = 2.0\n",
    "y = f_true + sigma_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"The true f(x)\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a sparse marginal likelihood model via `MarginalSparse`, where the approximation method can be chosen. We will use the **fully independent training conditional (FITC)** algorithm to estimate the model, with the critical approximation being the imposition of a conditional independence assumption on the joint prior over training and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sparse_model:\n",
    "    \n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.ExpQuad(1, ls)\n",
    "    gp = pm.gp.MarginalApprox(cov_func=cov, approx=\"FITC\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need a set of inducing points. We will initialize 20 inducing points with the **K-means** algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    \n",
    "    Xu = pm.gp.util.kmeans_inducing_points(20, X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we use the `marginal_likelihood` method, just as we did with the full GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    \n",
    "    sigma = pm.HalfCauchy(\"sigma\", 5)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X, Xu=Xu, y=y, noise=sigma, jitter=1e-5)\n",
    "    \n",
    "    trace = pm.sample(nuts_sampler='numpyro', chains=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"eta\", \"ls\"], backend_kwargs=dict(constrained_layout=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(-1, 11, 200)[:,None]\n",
    "\n",
    "# add the GP conditional to the model, given the new X values\n",
    "with sparse_model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    pred_samples = pm.sample_posterior_predictive(trace.sel(draw=slice(0, 5)), var_names=[\"f_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "f_pred_samples = az.extract(pred_samples, group=\"posterior_predictive\", var_names=[\"f_pred\"])\n",
    "plot_gp_dist(ax, f_pred_samples.T, X_new);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(Xu, 10*np.ones(Xu.shape[0]), \"co\", ms=10, label=\"Inducing point locations\")\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylim([-13,13]);\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilbert Space Approximate Gaussian Processes (HSGP)\n",
    "\n",
    "The Hilbert Space Gaussian processes approximation is a **low-rank GP approximation** that is particularly well-suited to usage in probabilistic programming languages.  It approximates the GP using a pre-computed and fixed set of basis functions that don't depend on the form of the covariance kernel or its hyperparameters.  It's a _parametric_ approximation, so prediction can be done as one would with any other type of Bayesian model in PyMC;  You don't need to define the `.conditional` distribution that the `Marginal` and `Latent` GPs rely on.  This makes it _much_ easier to integrate an HSGP into your existing model as they can be used anywhere within a model and with any likelihood function.  \n",
    "\n",
    "It's also fast!  The computational cost for standard GPs per MCMC step is $\\mathcal{O}(n^3)$, where $n$ is the number of data points.  For HSGPs, it is $\\mathcal{O}(mn + m)$, where $m$ is the number of basis vectors. \n",
    "\n",
    "The HSGP approximation does carry some caveats:\n",
    "1. It can only be used with _stationary_ covariance kernels such as the Matern family.  The `HSGP` class is compatible with any `Covariance` class that implements the `power_spectral_density` method.  There is a special case made for the `Periodic` covariance, which is implemented in PyMC by `HSGPPeriodic`. \n",
    "2. It does not scale well with the input dimension.  The HSGP approximation is a good choice if your GP is over a one dimensional process like a time series, or a two dimensional spatial point process.  It's likely not an efficient choice where the input dimension is larger than three. \n",
    "3. It _may_ struggle with more rapidly varying processes.  If the process you're trying to model changes very quickly relative to the extent of the domain, the HSGP approximation may fail to accurately represent it.  We'll show in later sections how to set the accuracy of the approximation, which involves a trade-off between the fidelity of the approximation and the computational complexity.\n",
    "4. For smaller data sets, the full unapproximated GP may still be more efficient.\n",
    "\n",
    "A secondary goal of this implementation is **flexibility** via an accessible implementation where the core computations are implemented in a modular way. This results in both a \"high-level\" interface where users can use the familiar `.prior` and `.conditional` methods and essentially treat the HSGP class as a drop in replacement for `pm.gp.Latent`, and a \"low-level\" interface that exposes the HSGP as a parametric model.  For more complex models with multiple HSGPs, users can work directly with functions like `pm.gp.hsgp_approx.calc_eigenvalues` and `pm.gp.hsgp_approx.calc_eigenvectors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "In HSGP, the covariance function is viewed as a mathematical operator acting on a high-dimensional space (Hilbert space). We represent covariance function as a series expansion of eigenvalues and eigenfunctions of the Laplace operator. These eigenfunctions capture the smoothness properties of the underlying function being modeled in the form of a set of basis functions.\n",
    "\n",
    "$$\n",
    "f \\sim \\mathcal{G P}\\left(0, k\\left(x, x^{\\prime} ; \\ell\\right)\\right) \\longrightarrow f \\approx \\phi(x) \\beta(\\ell)\n",
    "$$\n",
    "\n",
    "where the basis functions $\\phi$ only depend on the input and the coefficients $\\beta$ only depend on the kernel hyperparmeters.\n",
    "\n",
    "By approximating the covariance function with a set of basis functions, HSGP reduces the complexity significantly, from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(nm + m)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherry blossom dataset\n",
    "\n",
    "The data for this example is the number of days (`doy` for \"days of year\") that the cherry trees were in bloom in each year (`year`). \n",
    "For convenience, years missing a `doy` were dropped (which is a bad idea to deal with missing data in general!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    blossom_data = pd.read_csv(Path(\"..\", \"data\", \"cherry_blossoms.csv\"), sep=\";\")\n",
    "except FileNotFoundError:\n",
    "    blossom_data = pd.read_csv(pm.get_data(\"cherry_blossoms.csv\"), sep=\";\")\n",
    "\n",
    "\n",
    "blossom_data.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data = blossom_data.dropna(subset=[\"doy\"]).reset_index(drop=True)\n",
    "blossom_data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping rows with missing data, there are 827 years with the numbers of days in which the trees were in bloom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we visualize the data, it is clear that there a lot of annual variation, but some evidence for a non-linear trend in bloom days over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data.plot.scatter(\n",
    "    \"year\", \"doy\", color=\"cornflowerblue\", s=10, title=\"Cherry Blossom Data\", ylabel=\"Days in bloom\"\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on three columns: \n",
    "- `doy` the day of year of the first bloom\n",
    "- `temp` the temperature (since it goes so far back in time, I'm not sure how it's reconstructed)\n",
    "- `year` is of course the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = blossom_data[[\"year\", \"doy\", \"temp\"]].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 7)); axs = axs.flatten()\n",
    "\n",
    "axs[0].scatter(df[\"year\"], df[\"temp\"])\n",
    "axs[1].scatter(df[\"year\"], df[\"doy\"])\n",
    "\n",
    "axs[1].set_xlabel(\"year\")\n",
    "axs[0].set_ylabel(\"temp (C)\")\n",
    "axs[1].set_ylabel(\"day of the first bloom\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is the same as with any other GP model: we need to specify a covariance function. We will use the Matern 5/2 covariance function, which is a good default choice for many problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pz.maxent(pz.InverseGamma(), lower=20, upper=200, mass=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"year\": df.year.tolist()\n",
    "}\n",
    "with pm.Model(coords=coords) as cherry_blossom_model:\n",
    "    # Set prior on GP hyperparameters\n",
    "    eta = pm.Exponential(\"eta\", lam=0.25)\n",
    "\n",
    "    ell = pz.maxent(pz.InverseGamma(), lower=20, upper=200, mass=0.95, plot=False).to_pymc(\"ell\")\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify the HSGP model. We will use the `HSGP` class, which is a drop-in replacement for the `gp.Latent` class. \n",
    "\n",
    "There are three important hyperparameters for `HSGP`:\n",
    "\n",
    "- `m`: The **number of basis vectors** to use for each active dimension.\n",
    "- `L`: The **boundary of the space** for each active_dim. It is called the boundary condition. Choose `L` such that the domain `[-L, L]` contains all points in the column of `X` given by the active dimension.\n",
    "- `c`: The **proportion extension factor**. Used to construct `L` from `X`. Defined as $S = \\max|X| \\ni X \\in [-S, S]$. L is calculated as `c * S`. One of `c` or `L` must be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the HSGP approximation parameters\n",
    "\n",
    "Before fitting a model with an HSGP, you have to choose `m` and `c` or `L`.  `m` is the number of basis vectors.  Recall that the computational complexity of the HSGP approximation is $\\mathcal{O}(mn + m)$, where $n$ is the number of data points. \n",
    "\n",
    "This choice is a balance between three concerns:\n",
    "1.  The accuracy of the approximation.\n",
    "2.  Reducing the computational burden.\n",
    "3.  The `X` locations where predictions or forecasts will need to be made.\n",
    "\n",
    "The best way to understand how to choose these parameters is to understand how `m`, `c` and `L` relate to each other, which requires understanding a bit more about how the approximation works under the hood.  \n",
    "\n",
    "### How `L` and `c` affect the basis\n",
    "\n",
    "The HSGP approximates the GP prior as a **linear combination of sinusoids**.  The coefficients of the linear combination are IID normal random variables whose standard deviation depends on GP hyperparameters (which are an amplitude and lengthscale for the Matern family). \n",
    "\n",
    "To see this, we'll make a few plots of the $m=3$ basis vectors and pay careful attention to how they behave at the boundaries of the domain.  Note that we have to center the `x` data first, and then choose `L` in relation to the centered data.  It's worth mentioning here that the basis vectors we're plotting do not depend on either the choice of the covariance kernel or on any unknown parameters the covariance function has.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data goes from x=-5 to x=5\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# (plotting code)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "plt.subplots_adjust(wspace=0.02)\n",
    "ylim = 0.55\n",
    "axs[0].set_ylim([-ylim, ylim])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_ylim([-ylim, ylim])\n",
    "axs[1].set_xlabel(\"xs (mean subtracted x)\")\n",
    "axs[1].set_title(\"The effect of changing $L$ on the HSGP basis vectors\")\n",
    "axs[2].set_yticks([])\n",
    "axs[2].set_ylim([-ylim, ylim])\n",
    "\n",
    "# change L as we create the basis vectors\n",
    "L_options = [5.0, 6.0, 20.0]\n",
    "m_options = [3, 3, 5]\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    L = L_options[i]\n",
    "    m = m_options[i]\n",
    "\n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(pt.as_tensor([L]), [m])\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(\n",
    "        x[:, None], pt.as_tensor([L]), eigvals, [m]\n",
    "    ).eval()\n",
    "\n",
    "    colors = plt.cm.cividis_r(np.linspace(0.05, 0.95, m))\n",
    "    for j in range(phi.shape[1]):\n",
    "        ax.plot(x, phi[:, j], color=colors[j], label=f\"eigenvector {j+1}\")\n",
    "\n",
    "    ax.set_xticks(np.arange(-5, 6, 5))\n",
    "\n",
    "    S = 5.0\n",
    "    c = L / S\n",
    "    ax.text(-4.9, -0.45, f\"L = {L}\\nc = {c}\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both `L` and `m` are specified as lists, to allow setting `L` and `m` per input dimension.  In this example these are both one element lists since our example is in a one dimensional, time series like context.  Before continuing, it's helpful to define $S$ as the half range of the centered data, or the distance from the midpoint at $x=0$ to the edge, $x=5$.  In this example $S=5$ for each plot panel.  Then, we can define $c$ such that it relates $S$ to $L$, \n",
    "\n",
    "$$\n",
    "L = c \\cdot S \\,.\n",
    "$$\n",
    "It's usually easier to set $L$ by choosing $c$, which acts as a multiplier on $S$.  \n",
    "\n",
    "In the left-most plot we chose $L=S=5$, which is exactly on the edge of our `x` locations.  For any $m$, all the basis vectors are forced to pinch to zero at the edges, at $x=-5$ and $x=5$.  This means that the HSGP approximation becomes poor as you get closer to $x=-5$ and $x=5$.  How quickly depends on the lengthscale.  Large lengthscales require larger values of $L$ and $c$, and smaller lengthscales attenuate this issue.  Ruitort-Mayol *et al.* recommend using 1.2 as a minimum value.  The effect of this choice on the basis vectors is shown in the center panel.  \n",
    "\n",
    "The right panel shows the effect of choosing a larger $L$, or setting $c=4$.  Larger values of $L$ or $c$ make the boundary conditions less problematic, and are required to accurately approximate GPs with longer lengthscales.  You also need to consider where predictions will need to be made.  In addition to the locations of the observed $x$ values, the locations of the new $x$ locations also need to be away from the \"pinch\" caused by the boundary condition.  The _period_ of the basis functions also increases as we increase $L$ or $c$.  This means that we will need to increase $m$ in order to compensate if we wish to approximate GPs with smaller lengthscales.  \n",
    "\n",
    "With large $L$ or $c$, the first eigenvector can flatten so much that it becomes partially or completely unidentifiable with the intercept in the model.  The right-most panel is an example of this.  It can be very beneficial to sampling to drop the first eigenvector in these situations.  The `HSGP` and `HSGPPeriodic` class in PyMC both have the option `drop_first` to do this, or if you're using `.prior_linearized` you can control this yourself.  Be sure to check the basis vectors if the sampler is having issues.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "- Increasing $m$ helps the HSGP approximate GPs with smaller lengthscales, at the cost of increasing computational complexity\n",
    "- Increasing $c$ or $L$ helps the HSGP approximate GPs with larger lengthscales, but may require increasing $m$ to compensate for the loss of fidelity at smaller lengthscales.  \n",
    "- When choosing $m$, $c$ or $L$, it's important to consider the locations where you will need to make predictions, such that they also aren't affected by the boundary condition.\n",
    "- The first eigenvector in the basis may be unidentified with the intercept, especially when $L$ or $c$ are larger. \n",
    "\n",
    "\n",
    "### Heuristics for choosing $m$ and $c$\n",
    "\n",
    "In practice, you'll need to infer the lengthscale from the data, so the HSGP needs to approximate a GP across a range of lengthscales that are representative of your chosen prior.  You'll need to choose $c$ large enough to handle the largest lengthscales you might fit, and also choose $m$ large enough to accommodate the smallest lengthscales.  Ruitort-Mayol *et al.* give some handy heuristics for the range of lengthscales that are accurately reproduced for given values of $m$ and $c$.  The plot below uses their heuristics to recommend minimum $m$ and $c$ value.  Note that these recommendations are based on a one-dimensional GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "c_list = np.array([1.2, 1.3, 1.5, 1.75, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0])\n",
    "ell = np.linspace(1, 1000, 500)\n",
    "S = (df[\"year\"] - df[\"year\"].mean()).max() # half-range of the input data\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "cmap = plt.cm.cividis\n",
    "colors = np.arange(len(c_list)) / len(c_list)\n",
    "\n",
    "for i, c in enumerate(c_list):\n",
    "    m = 2.65 * (c / ell) * S\n",
    "   \n",
    "    ix = c >= (4.1 * (ell / S))\n",
    "    m[~ix] = np.nan\n",
    "    ax.semilogy(ell, m, color=cmap(colors[i]), label=\"c = %s\" % str(c), lw=3);\n",
    "\n",
    "ax.grid(True);\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(50))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "\n",
    "ax.set_title(\"Matern52 HSGP approx parameter curves\");\n",
    "ax.set_ylim([10, 1000]);\n",
    "ax.set_xlabel(\"lengthscale ()\");\n",
    "ax.set_ylabel(\"number of basis vectors (m)\");\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves show the regions where the HSGP approximation is accurate.  For instance, the darkest blue line at $c=1.2$ means that if we choose $c=1.2$, our approximation will be valid for the smallest lengthscales up until about $\\ell = 130$.  The yellow curve at $c=6.0$ means shows that the approximation is accurate up until about $\\ell = 650$.  **How we choose these values depends on our prior for the lengthscale**.  In our case we are setting 95% of the prior mass between 20 and 200.  We'll be safe and choose $c=2$ and $m=400$ so our prior range is well covered pretty well.  \n",
    "\n",
    "Also, keep in mind that HSGP scales as $\\mathcal{O}(nm + m)$, so the smaller we can choose $m$, the better, speed-wise.  $c$ plays no role in computation speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `HSGP` in PyMC\n",
    "\n",
    "There are a couple of usage details that are important for using `HSGP` effectively. The first is that, due to how the basis vectors are constructed, the `HSGP` class requires that the `X` input data be centered.  This is because the basis vectors are constructed to be periodic over the range $[-L, L]$, and the center of the range is assumed to be zero.  \n",
    "\n",
    "So, here we will center the age range on the mean age, taking care to use the closest integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df['year'].values\n",
    "years_centered = years - years[len(years) // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cherry_blossom_model:\n",
    "\n",
    "    gp = pm.gp.HSGP(m=[400], c=2.0, cov_func=cov, parametrization='non-centered')\n",
    "    f = gp.prior(\"f\", X=years_centered[:, None], dims=\"year\")\n",
    "    \n",
    "    intercept = pm.Normal(\"intercept\", mu=df[\"temp\"].mean(), sigma=2 * df[\"temp\"].std())\n",
    "    mu = pm.Deterministic(\"mu\", intercept + f, dims=\"year\")\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=2 * df[\"temp\"].std())\n",
    "    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=df[\"temp\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the specification of `HSGP` the parameterization was set to **non-centered**. This relates to how the power spectrum is entered into the model. Here is what happens under the hood:\n",
    "\n",
    "First, `prior_linearized` returns the eigenvector basis, `phi`, and the square root of the power spectrum at the eigenvalues, `sqrt_psd`.  You have to construct the HSGP approximation from these.  The following are the relevant lines of code, showing both the centered and non-centered parameterization.\n",
    "\n",
    "```python\n",
    "phi, sqrt_psd = gp.prior_linearized(Xs=Xs)\n",
    "\n",
    "## non-centered\n",
    "beta = pm.Normal(\"beta\", size=gp._m_star)\n",
    "f = pm.Deterministic(\"f\", phi @ (beta * sqrt_psd)) \n",
    "\n",
    "## centered\n",
    "beta = pm.Normal(\"beta\", sigma=sqrt_psd, size=gp._m_star)\n",
    "f = pm.Deterministic(\"f\", phi @ beta) \n",
    "```\n",
    "\n",
    "Choosing a non-centered parameterization may help alleviate sampling issues, but is not always necessary, particularlty if there are plenty of data informing the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cherry_blossom_model:\n",
    "    trace_gp = pm.sample(nuts_sampler='numpyro', chains=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_gp, var_names=[\"eta\", \"ell\", \"sigma\", \"intercept\"])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a few minutes on my machine. We see a lengthscale of about 40, which we can interpret as meaning that it takes about 40 years for significant changes in temperature to occur.  We can also interpret this to roughly mean that the GP is comfortable predicting out about 40 years.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(18, 10));\n",
    "\n",
    "f = az.extract(trace_gp, group=\"posterior\", var_names=\"mu\")\n",
    "pm.gp.util.plot_gp_dist(ax=ax, samples=f.values.T, x=df[\"year\"].values)\n",
    "ax.plot(df[\"year\"].values, df[\"temp\"].values, \"ok\", alpha=0.25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HSGP approximation does carry some restrictions:\n",
    "1. It **can only be used with _stationary_ covariance kernels** such as the Matern family.  The `HSGP` class is compatible with any `Covariance` class that implements the `power_spectral_density` method.  There is a special case made for the `Periodic` covariance, which is implemented in PyMC by The `HSGPPeriodic`.\n",
    "2. It **does not scale well with the input dimension**.  The HSGP approximation is a good choice if your GP is over a one dimensional process like a time series, or a two dimensional spatial point process.  It's likely not an efficient choice where the input dimension is larger than three. \n",
    "3. It **_may_ struggle with more rapidly varying processes**.  If the process you're trying to model changes very quickly relative to the extent of the domain, the HSGP approximation may fail to accurately represent it.  \n",
    "4. **For smaller data sets, the full unapproximated GP may still be more efficient**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "[Rasmussen, C. E., & Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning series). The MIT Press.](http://www.amazon.com/books/dp/026218253X)\n",
    "\n",
    "[Quinonero-Candela, J. & Rasmussen, C. E. (2005). A Unifying View of Sparse Approximate Gaussian Process Regression\n",
    "Journal of Machine Learning Research 6, 19391959.](http://www.jmlr.org/papers/v6/quinonero-candela05a.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "gist": {
   "data": {
    "description": "GP Showdown.ipynb",
    "public": true
   },
   "id": "05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
